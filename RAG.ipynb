{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders and Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "#loader = TextLoader(\"./\")\n",
    "loader = PyPDFLoader(\"미국주식_Test.pdf\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnstructuredFileLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "\n",
    "loader = UnstructuredFileLoader(\"미국주식_Test.pdf\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnstructuredFileLoader, RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(# split when sentence ending or paragraph ending\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"미국주식_Test.pdf\")\n",
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loader.load_and_split(text_splitter=splitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Text splitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter( # split with specifit characters\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"미국주식_Test.pdf\")\n",
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loader.load_and_split(text_splitter=splitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator='\\n',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"미국주식_Test.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "embedder.embed_query('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension    \n",
    "len(embedder.embed_query('Hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embedder.embed_documents(['Hi','how','are','you'])\n",
    "print(len(vector),len(vector[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstores_Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings # text-embedding-ada-002\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator='\\n',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"미국주식_Test.pdf\")\n",
    "docs = loader.load_and_split(text_splitter=splitter) \n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(docs, embeddings) # pass doc and openai embbeding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"절대가치 평가에서 중요한 것은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"절대가치 평가에서 중요한 것은?\")\n",
    "len(results)\n",
    "# this is why chunking is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings # Cache\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore #local storage\n",
    "\n",
    "cache_dir = LocalFileStore('./.cache')\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator='\\n',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/미국주식_Test.pdf\")\n",
    "docs = loader.load_and_split(text_splitter=splitter) \n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings) #이제 cached embedding 에서 (Not OpenAI Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문서에는 회사 운영의 효율성을 측정하는 방법에 대한 구체적인 내용은 포함되어 있지 않습니다. 이에 대해 알려드릴 수 있는 정보가 없습니다. 해당 내용에 대해 더 자세히 알고 싶으시다면, 다른 출처를 참고하시기를 권장드립니다.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings \n",
    "from langchain.vectorstores import Chroma #FAISS로 바꾸어도 돼. 단, cache지우고\n",
    "from langchain.storage import LocalFileStore \n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore('./.cache') \n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator='\\n',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/미국주식_Test.pdf\")    # load file (any kinds)\n",
    "docs = loader.load_and_split(text_splitter=splitter)  # split into smaller docs ; send llm chopped ones\n",
    "embeddings = OpenAIEmbeddings() # OpenAI's embedding\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(  # cached the embeddings (not free though)\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings) #FAISS로 바꾸어도 돼  # perform searches and get related docs\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(  # RetrievalQA : \n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\", # many different types, check below\n",
    "    retriever=vectorstore.as_retriever(), #vectorstore is one of the retriever\n",
    ")\n",
    "\n",
    "chain.run(\"회사 운영 본연의 측면에서 회사 경영의 효율성을 측정하는 방법은? 문서에 없는 내용이면 모른다고 말해\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-A central question for building a summarizer is how to pass your documents into the LLM's context window. Three common approaches for this are:\n",
    "\n",
    "1. Stuff: Simply \"stuff\" all your documents into a single prompt. This is the simplest approach (see here for more on the create_stuff_documents_chain constructor, which is used for this method).\n",
    "- 검색을 통해 관련 문서들을 얻어서 prompt입력 한후, llm에게 질문에 답하세요라고 전달.\n",
    "    \n",
    "2. Map-reduce: Summarize each document on its own in a \"map\" step and then \"reduce\" the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method).\n",
    "- Document를 각각 보면서, 개별 답변을 각각 docs에서 찾고, 탐색이 끝나면 일종의 중간 답변들을 기반으로 최종 답 생성\n",
    "\n",
    "3. Refine: Update a rolling summary be iterating over the documents in a sequence.\n",
    "- 다큐멘트 최초 답변이 있고, 그것을 고쳐나가는 방식\n",
    "\n",
    "4. Map_rerank: 각 다튜면트 돌아보고, 각 docs에서 질문 답하고, score하고 제일 높은 점수 답변 제공\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff LCEL Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://python.langchain.com/v0.1/docs/expression_language/interface/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='절대 가치 평가, 비교 가치 평가, 재무제표 가치 평가와 같은 방법들을 사용하여 회사의 경영 효율성을 측정할 수 있습니다.', response_metadata={'token_usage': <OpenAIObject at 0x7fcd8362da30> JSON: {\n",
       "  \"prompt_tokens\": 1242,\n",
       "  \"completion_tokens\": 62,\n",
       "  \"total_tokens\": 1304\n",
       "}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-301846b4-0ffe-4a33-a9da-d531e3dd9ad3-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings \n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.storage import LocalFileStore \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "cache_dir = LocalFileStore('./.cache') \n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator='\\n',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/미국주식_Test.pdf\")  \n",
    "docs = loader.load_and_split(text_splitter=splitter)  \n",
    "embeddings = OpenAIEmbeddings() \n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(  \n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings) \n",
    "\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Answer questions using only the following context. \\\n",
    "         If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    " \n",
    "chain = ({\"context\" : retriever,  # retriver is one of the component of chain; will receive string and output the list of docs. 즉  retriever(\"회사 운영 본연의 측면에서 회사 경영의 효율성을 측정하는 방법은? 문서에 없는 내용이면 모른다고 말해\") 이게 output [docs]\n",
    "         \"question\": RunnablePassthrough()} # passthrough \"\"회사 운영 본연의 측면에서 회사 경영의 효율성을 측정하는 방법은? 문서에 없는 내용이면 모른다고 말해\"\n",
    "        | prompt | llm)  #2. retriever will give the list of the docs 3. a) 이 docs들이 system {context}로 가고 b) {question}가고\n",
    "\n",
    "chain.invoke(\"회사 운영 본연의 측면에서 회사 경영의 효율성을 측정하는 방법은? 문서에 없는 내용이면 모른다고 말해\") # 1. 이게 retriver로 가고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20079dd2ac21832bb0e8ff352d23b6c3837ca9c91609572aa5b291e173833a08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
