{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders and Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "#loader = TextLoader(\"./\")\n",
    "loader = PyPDFLoader(\"미국주식_Test.pdf\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnstructuredFileLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "\n",
    "loader = UnstructuredFileLoader(\"미국주식_Test.pdf\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnstructuredFileLoader, RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(# split when sentence ending or paragraph ending\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"미국주식_Test.pdf\")\n",
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loader.load_and_split(text_splitter=splitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Text splitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter( # split with specifit characters\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"미국주식_Test.pdf\")\n",
    "loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loader.load_and_split(text_splitter=splitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator='\\n',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"미국주식_Test.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "embedder.embed_query('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension    \n",
    "len(embedder.embed_query('Hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embedder.embed_documents(['Hi','how','are','you'])\n",
    "print(len(vector),len(vector[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstores_Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings # text-embedding-ada-002\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator='\\n',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"미국주식_Test.pdf\")\n",
    "docs = loader.load_and_split(text_splitter=splitter) \n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(docs, embeddings) # pass doc and openai embbeding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"절대가치 평가에서 중요한 것은?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"절대가치 평가에서 중요한 것은?\")\n",
    "len(results)\n",
    "# this is why chunking is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings # Cache\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore #local storage\n",
    "\n",
    "cache_dir = LocalFileStore('./.cache')\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator='\\n',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/미국주식_Test.pdf\")\n",
    "docs = loader.load_and_split(text_splitter=splitter) \n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings) #이제 cached embedding 에서 (Not OpenAI Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문서에는 회사 운영의 효율성을 측정하는 방법에 대한 구체적인 내용은 포함되어 있지 않습니다. 이에 대해 알려드릴 수 있는 정보가 없습니다. 해당 내용에 대해 더 자세히 알고 싶으시다면, 다른 출처를 참고하시기를 권장드립니다.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings \n",
    "from langchain.vectorstores import Chroma #FAISS로 바꾸어도 돼. 단, cache지우고\n",
    "from langchain.storage import LocalFileStore \n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "cache_dir = LocalFileStore('./.cache')\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator='\\n',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/미국주식_Test.pdf\")\n",
    "docs = loader.load_and_split(text_splitter=splitter) \n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "vectorstore = Chroma.from_documents(docs, cached_embeddings) #FAISS로 바꾸어도 돼\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\", #\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "\n",
    "chain.run(\"회사 운영 본연의 측면에서 회사 경영의 효율성을 측정하는 방법은? 문서에 없는 내용이면 모른다고 말해\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-A central question for building a summarizer is how to pass your documents into the LLM's context window. Three common approaches for this are:\n",
    "\n",
    "1. Stuff: Simply \"stuff\" all your documents into a single prompt. This is the simplest approach (see here for more on the create_stuff_documents_chain constructor, which is used for this method).\n",
    "\n",
    "2. Map-reduce: Summarize each document on its own in a \"map\" step and then \"reduce\" the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method).\n",
    "\n",
    "3. Refine: Update a rolling summary be iterating over the documents in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20079dd2ac21832bb0e8ff352d23b6c3837ca9c91609572aa5b291e173833a08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
