# OllmaëŠ” ë‚´ ë¡œì»¬ì— ì„œë²„ ìƒì„±
# terminal : source env/bin/activate
# terminal : streamlit run Home.py
# to kill the server : ctl + c
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOllama
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st

st.set_page_config(
    page_title="PrivateGPT",
    page_icon="ğŸ“ƒ",
)

#ChatCallbackHandler : llmì—ì„œ ì–´ë–¤ì¼ì´ ë°œìƒeventí•˜ë©´ ëª¨ë“  methodë“¤ì´ í˜¸ì¶œëœë‹¤. 
class ChatCallbackHandler(BaseCallbackHandler):
    message = ""  # LLMì´ ì‚¬ì§ë˜ë©´  empty textìƒì„±
    # llmì´ ì–¸ì œ í† í° ìƒì„± ì‹œì‘í•˜ëŠ”ì§€ 
    def on_llm_start(self, *args, **kwargs): # can have unlimited args and key word arg
        self.message_box = st.empty() # LLMì´ ì‚¬ì§ë˜ë©´  empty boxìƒì„±
    # llmì´ ì–¸ì œ ëë‚˜ëŠ”ì§€, full messageê°€ ë˜ë©´ 
    def on_llm_end(self, *args, **kwargs): 
        save_message(self.message, "ai")
    # í† í°ì„ messageì— í•©ì³
    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token 
        self.message_box.markdown(self.message) # ë©”ì‹œì§€ë¥¼ ë°©ê¸ˆ ì—…ë°ì´íŠ¸ í•œ ë©”ì„¸ì§€ì™€ markdown textë¡œ ì œê³µ


llm = ChatOllama(
    model="mistral:latest",
    temperature=0.1,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(), # ê¸°ë³¸ì ìœ¼ë¡œ llmì˜ eventë¥¼ listení•˜ëŠ” class
    ],
)

# ì´ computationì„ cacheí•œë‹¤. íŒŒì¼ì´ ë³€ê²½ë˜ì§€ ì•ŠëŠ” í•œ, ë™ì¼í•œ íŒŒì¼ì„ í•¨ìˆ˜ì— ê³„ì† ë³´ë‚´ë©´ì„œ í•¨ìˆ˜ëŠ” ì‘ë™ë˜ì§€ ì•ŠìŒ
@st.cache_resource(show_spinner="Embedding file...") 
def embed_file(file):  
    file_content = file.read() # íŒŒì¼ ì½ê³ 
    file_path = f"./.cache/private_files/{file.name}" # ì €ì¥í•˜ê³ 
    with open(file_path, "wb") as f:
        f.write(file_content)
    cache_dir = LocalFileStore(f"./.cache/private_embeddings/{file.name}")#ì €ì¥í•˜ê³ 
    splitter = CharacterTextSplitter.from_tiktoken_encoder( #splití•˜ê³ 
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    loader = UnstructuredFileLoader(file_path) # load
    docs = loader.load_and_split(text_splitter=splitter) # doc split
    embeddings = OllamaEmbeddings(model='mistral:latest') #embed
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir) # cachedëœ ê±°ì—ì„œ ì„ë² ë”©ì„ ê°€ì ¸ì˜¤ê³ 
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()#ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¦¬íŠ¸ë¦¬ë²„ë¡œ ë°”ê¾¸ê³  
    return retriever # ì²´ì¸ìœ¼ë¡œ ê°„ë‹¤; ê²°êµ­ ê·¸ëƒ¥ ë‹¤íë¨¼íŠ¸ë¥¼ ì£¼ë ¤ëŠ”ê²Œ ëª©ì  ë‹¨ìˆœ


def save_message(message, role): # ë”•ì…”ë„ˆë¦¬ë¥¼ ë”í•˜ëŠ”  gkatn
    st.session_state["messages"].append({"message": message, "role": role})


def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role) # ë©”ì„¸ì§€ ì €ì¥


def paint_history(): #message storeì˜ ëª¨ë“  ë©”ì„¸ì§€ì— ëŒ€í•´ì„œ ì´ send_messageë¥¼ í†µí•´ í™”ë©´ì— ì¶œë ¥
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False, # storeì— ì €ì¥í•˜ì§€ ì•Šê³ (ì´ë¯¸ ì €ì¥ëœê²ƒë“¤ì´ë‹ˆê¹Œ)
        )


def format_docs(docs): # stringìœ¼ë¡œ docì„ ì¤„ê²ƒì´ì•¼ (linebreak í•´ì„œ)
    return "\n\n".join(document.page_content for document in docs)


prompt = ChatPromptTemplate.from_template(
        """
        Answer the question using ONLY the following context and not your training data. If you don't know the answer just say you don't know. DON'T make anything up.
        Context: {context} 
        Question: {question}
        """ 
        )



st.title("PrivateGPT")

st.markdown(
    """
Welcome!
            
Use this chatbot to ask questions to an AI about your files!

Upload your files on the sidebar.
"""
)
# 1_1. File upload in the side bar
with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )
# 1_3. userê°€ íŒŒì¼ì„ ì—…ë¡œë“œ í•˜ë©´ ìš°ë¦¬ëŠ” íŒŒì¼ë¡œë¶€í„° retrieverì–»ìŒ
if file:
    retriever = embed_file(file) # embed_file í•¨ìˆ˜ ê°€ë™ 
    send_message("I'm ready! Ask away!", "ai", save=False) # send_message ê°€ë™
    paint_history() #paint_historyì‘ë™ 
    message = st.chat_input("Ask anything about your file...") # ì‚¬ìš©ìê°€ ì§ˆë¬¸í•  chat inputìƒì„±
    if message: # 
        send_message(message, "human") #ë©”ì„¸ì§€ë¥¼ í™”ë©´ì— í‘œì‹œí•˜ê³  ì €ì¥
        chain = (
            { 
                "context": retriever | RunnableLambda(format_docs), #'retriever'ì—ì„œ list of docë“¤ ì¤„ê²ƒì´ê³  --> format_docs
                "question": RunnablePassthrough(), #invoke í•˜ë©´ messages (ì‚¬ìš©ì ì§ˆë¬¸)
            }
            | prompt 
            | llm
        )
        with st.chat_message("ai"): #ëŒ€ë‹µì„ ì—ì´ì•„ì´ ì•„ì´ì½˜ ì˜†ì—ì„œ
            response = chain.invoke(message)

# 1_2. ì²˜ìŒì—ëŠ” íŒŒì¼ì´ ì—†ì„ ê²ƒì„, ê·¸ëŸ¬ë©´ session stateì— ìˆëŠ” ë©”ì„¸ì§€ ì €ì¥ì†Œë¥¼ empty list ë¡œ ì´ˆê¸°í™”
else:
    st.session_state["messages"] = []