{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chulwonchae/FullStack_GPT/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "/Users/chulwonchae/FullStack_GPT/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' in the solar system\\nThere are eight planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. In addition, there are five dwarf planets: Ceres, Pluto, Haumea, Makemake, and Eris.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Language Models\n",
    "from langchain.llms.openai import OpenAI #llm 모델 선택 (davinci 는 비싸)\n",
    "from langchain.chat_models import ChatOpenAI #chat 모델 선택, #ChatAnthropic\n",
    "\n",
    "llm = OpenAI()\n",
    "#chat = ChatOpenAI() # ChatAnthropic() 모델 바로 바꿀수 있어\n",
    "# chat모델은 chat에 특화\n",
    "a = llm.predict(\"How many planets are there\")\n",
    "#b = chat.predict(\"How many planets are there\") \n",
    "a\n",
    "#b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chulwonchae/FullStack_GPT/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/Users/chulwonchae/FullStack_GPT/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are eight planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAI #llm 모델 선택 (davinci 는 비싸)\n",
    "from langchain.chat_models import ChatOpenAI #chat 모델 선택, #ChatAnthropic\n",
    "\n",
    "#llm = OpenAI()\n",
    "chat = ChatOpenAI() # ChatAnthropic() 모델 바로 바꿀수 있어\n",
    "# chat모델은 chat에 특화\n",
    "# a = llm.predict(\"How many planets are there\")\n",
    "b = chat.predict(\"How many planets are there\") \n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt template\n",
    "- Prompt templates help to translate user input and parameters into instriction for a language model. \n",
    "- Can be used to guid a model's response, helping it understand the context and generate relevant and coherent language based output.\n",
    "- The output of prompt templates PromptValue, and this will be passed to an LLM or a ChatModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The distance between Mexico and Thailand is approximately 16,000 kilometers (9,942 miles). My name is Assistant.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI #chat 모델 선택, #ChatAnthropic\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate #ChatPromptTemplate : 은 아래 'messages'로 부터 프롬프트를 만들고 / PromptTemplate : 은 string으로부터 만듬\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1) \n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the distance between {place_a} and {place_b}? Also, what is your name?\")\n",
    "prompt = template.format(place_a='Mexico', place_b='Thailand')\n",
    "\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatPromptTemplates\n",
    "- These prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predict message\n",
    "# from langchain.schema import HumanMessage, AIMessage, SystemMessage #system message : 우리가 llm설정들을 제공하기 위한 message\n",
    "# messages = [\n",
    "#     SystemMessage(content = \"You are a geography expert. And you only reply in {language}\"),\n",
    "#     AIMessage(content = \"안녕하세요, 저는 대한민국 대동여지도를 만든 {name} 입니다.\"),\n",
    "#     HumanMessage(content = \"What is the distance between {place_a} and {place_b}? Also, what is your name?\")\n",
    "# ]\n",
    "\n",
    "# chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"system\": 시스템 메시지를 나타내며, 주로 초기 설정이나 규칙을 지정하는 데 사용됩니다.\n",
    "- \"ai\": 인공지능 모델이 생성한 응답을 나타냅니다.\n",
    "- \"human\": 사용자(또는 인간)가 입력한 메시지를 나타냅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Γεια σας! Το όνομά μου είναι Σωκράτης. Η απόσταση μεταξύ του Μεξικού και της Ταϊλάνδης είναι περίπου 16.000 χιλιόμετρα.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tempplate 생성\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert. And you only reply in {language}\"),\n",
    "    (\"ai\", \"안녕하세요, 저는 대한민국 대동여지도를 만든 {name} 입니다.\"),\n",
    "    (\"human\", \"What is the distance between {place_a} and {place_b}? Also, what is your name?\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    language = \"Greek\",\n",
    "    name = \"Socrates\",\n",
    "    place_a = \"Mexico\",\n",
    "    place_b = \"Thailand\"\n",
    ")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output Parser필요한 이유는 llm의 응답을 변형해야할 때가 있기 때문 (리스트로 변환하던지)\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    \n",
    "    def parse(self,text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "    \n",
    "p = CommaOutputParser()\n",
    "p.parse(\"Hello,how, are,you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'purple',\n",
       " 'pink',\n",
       " 'black',\n",
       " 'white',\n",
       " 'brown']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",\"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items}. Do NOT reply with anything else.\",),\n",
    "     (\"human\",\"{question}\"),]\n",
    ")\n",
    "prompt = template.format_messages(max_items=10, question=\"What are the colors?\")\n",
    "result = chat.predict_messages(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "p.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pikachu', 'Charmander', 'Bulbasaur', 'Squirtle', 'Jigglypuff']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better way.\n",
    "# 코드를 다시 정리 해보자 (여기서 부터 코드 다시 돌려봐)\n",
    "chat = ChatOpenAI(temperature=0.1) \n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "    \n",
    "    def parse(self,text):\n",
    "        items = text.strip().split(\",\")\n",
    "        return list(map(str.strip, items))\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",\"You are a list generating machine. Everything you are asked will be answered with a comma separated list of max {max_items}. Do NOT reply with anything else.\",),\n",
    "     (\"human\",\"{question}\"),]\n",
    ")\n",
    "\n",
    "# Chain을 만들거야\n",
    "chain = template | chat | CommaOutputParser()\n",
    "chain.invoke({  #chain 실행, dictionary형태로\n",
    "    \"max_items\" : 5,\n",
    "    \"question\" : \"What are the Pokemons\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 chain = chain +chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 낮을수록 창의적\n",
    "    streaming=True,  # 실시간으로 글자 보여줘 \n",
    "    callbacks=[StreamingStdOutCallbackHandler()]) # console에 보이는 문자를 다 print해줘 (다양한 llm에서 일어나는 event를 감지하는 방법)\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class international chef. You create easy to follow recipies for any types of cuisine with easy to find ingredients.\"),\n",
    "    (\"human\", \"I want to cook {cuisine} food.\"),\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great choice! Indian cuisine is known for its bold flavors and aromatic spices. Let's start with a classic and popular Indian dish - Chicken Tikka Masala. Here's a simple recipe for you to try at home:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb boneless, skinless chicken breasts, cut into bite-sized pieces\n",
      "- 1 cup plain yogurt\n",
      "- 2 tablespoons lemon juice\n",
      "- 2 teaspoons ground cumin\n",
      "- 2 teaspoons ground coriander\n",
      "- 1 teaspoon turmeric\n",
      "- 1 teaspoon garam masala\n",
      "- 1 teaspoon paprika\n",
      "- 1 teaspoon chili powder (adjust to taste)\n",
      "- 3 cloves garlic, minced\n",
      "- 1 tablespoon grated ginger\n",
      "- Salt and pepper to taste\n",
      "- 2 tablespoons oil\n",
      "- 1 onion, diced\n",
      "- 1 can (14 oz) tomato sauce\n",
      "- 1 cup heavy cream\n",
      "- Fresh cilantro, chopped (for garnish)\n",
      "\n",
      "Instructions:\n",
      "1. In a bowl, combine yogurt, lemon juice, cumin, coriander, turmeric, garam masala, paprika, chili powder, garlic, ginger, salt, and pepper. Add the chicken pieces and mix well to coat. Marinate for at least 1 hour, or overnight for best results.\n",
      "\n",
      "2. Heat oil in a large skillet over medium-high heat. Add the diced onion and cook until softened, about 5 minutes.\n",
      "\n",
      "3. Add the marinated chicken pieces to the skillet, along with any excess marinade. Cook until the chicken is browned on all sides, about 5-7 minutes.\n",
      "\n",
      "4. Stir in the tomato sauce and bring to a simmer. Reduce heat to low, cover, and cook for 15-20 minutes, stirring occasionally.\n",
      "\n",
      "5. Stir in the heavy cream and simmer for an additional 5 minutes.\n",
      "\n",
      "6. Taste and adjust seasoning if needed. Serve the Chicken Tikka Masala over steamed rice, garnished with fresh cilantro.\n",
      "\n",
      "Enjoy your homemade Chicken Tikka Masala with naan bread or rice! Feel free to customize the spice level to suit your taste preferences.To make a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a suitable alternative. One great option is to use paneer, which is a type of Indian cottage cheese that holds its shape well when cooked. Here's how you can prepare the paneer for this recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb paneer, cut into bite-sized cubes\n",
      "\n",
      "Instructions:\n",
      "1. In a bowl, combine yogurt, lemon juice, cumin, coriander, turmeric, garam masala, paprika, chili powder, garlic, ginger, salt, and pepper as per the original recipe.\n",
      "2. Add the paneer cubes to the marinade and mix gently to coat. Allow the paneer to marinate for at least 1 hour, or overnight for best results.\n",
      "3. Follow the remaining steps of the recipe as directed, substituting the marinated paneer for the chicken.\n",
      "4. When cooking the paneer in the skillet, be gentle to avoid breaking the cubes. Cook until the paneer is lightly browned on the edges.\n",
      "\n",
      "By using paneer as a replacement for chicken, you can enjoy a flavorful and satisfying vegetarian version of Chicken Tikka Masala. Serve it with rice or naan bread for a delicious meal!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"To make a vegetarian version of Chicken Tikka Masala, we can replace the chicken with a suitable alternative. One great option is to use paneer, which is a type of Indian cottage cheese that holds its shape well when cooked. Here's how you can prepare the paneer for this recipe:\\n\\nIngredients:\\n- 1 lb paneer, cut into bite-sized cubes\\n\\nInstructions:\\n1. In a bowl, combine yogurt, lemon juice, cumin, coriander, turmeric, garam masala, paprika, chili powder, garlic, ginger, salt, and pepper as per the original recipe.\\n2. Add the paneer cubes to the marinade and mix gently to coat. Allow the paneer to marinate for at least 1 hour, or overnight for best results.\\n3. Follow the remaining steps of the recipe as directed, substituting the marinated paneer for the chicken.\\n4. When cooking the paneer in the skillet, be gentle to avoid breaking the cubes. Cook until the paneer is lightly browned on the edges.\\n\\nBy using paneer as a replacement for chicken, you can enjoy a flavorful and satisfying vegetarian version of Chicken Tikka Masala. Serve it with rice or naan bread for a delicious meal!\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a vegetarian chef specialized on making traditional recipies vegetarian. You find alternative ingredients and explain their preparation. You don't radically modifiy the recipe. If there is no alternative for a food just say you don't know how to replace it.\"),\n",
    "    (\"human\", \"{recipe}\"),\n",
    "])\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain # \"recipe\" context object를 체인에 넣어줘 (아래 코드 안쓰고, recipe 값에 결과 저장 후, 다음 체인으로 전달)\n",
    "final_chain.invoke({ \n",
    "    \"cuisine\":\"indian\"\n",
    "})\n",
    "\n",
    "#\"recipe\": chef_chain 이것을 함으로서 자동으로 veg_chain.invoke() 가 돌아가는 것임\n",
    "\n",
    "# # 즉, chef_chain의 결과 {recipe}를 veg_chain의 input 으로..\n",
    "# veg_chain.invoke({\n",
    "#     \"recipe\":\"chatmodel\" #이런식으로\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모듈\n",
    "# LangChain은 다음과 같은 주요 모듈을 위한 확장 가능한 표준 인터페이스 및 외부 통합 기능을 제공합니다:\n",
    "\n",
    "# - Model I/O\n",
    "# 언어 모델과의 인터페이스\n",
    "\n",
    "# - Retrieval\n",
    "# 애플리케이션별 데이터를 사용한 인터페이스\n",
    "\n",
    "# - Agents\n",
    "# 높은 수준의 지침이 주어지면 체인이 어떤 도구를 사용할지 선택하도록 합니다\n",
    "\n",
    "# - Chains\n",
    "# 일반적인 빌딩 블록 구성\n",
    "\n",
    "# - Memory\n",
    "# 체인 실행 간 지속적인 응용 프로그램 상태\n",
    "\n",
    "# - Callbacks\n",
    "# 모든 체인의 중간 단계 기록 및 스트리밍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# few shot \n",
    "# you are giving examples to model how to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of France'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 낮을수록 창의적\n",
    "    streaming=True,  # 실시간으로 글자 보여줘 \n",
    "    callbacks=[StreamingStdOutCallbackHandler()]) # console에 보이는 문자를 다 print해줘 (다양한 llm에서 일어나는 event를 감지하는 방법)\n",
    "\n",
    "t = PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "t.format(country='France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #이 형식도 똑같아 (langchain document way)\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.prompts import ChatMessagePromptTemplate\n",
    "# from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "# from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# chat = ChatOpenAI(\n",
    "#     temperature=0.1, # 낮을수록 창의적\n",
    "#     streaming=True,  # 실시간으로 글자 보여줘 \n",
    "#     callbacks=[StreamingStdOutCallbackHandler()]) # console에 보이는 문자를 다 print해줘 (다양한 llm에서 일어나는 event를 감지하는 방법)\n",
    "\n",
    "# t = PromptTemplate(\n",
    "#     template = \"What is the capital of {country}\",\n",
    "#     input_variables = ['country'])\n",
    "\n",
    "# t.format(country='France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n",
      "        Here is what I know:\n",
      "        Capital: Berlin\n",
      "        Language: German\n",
      "        Food: Bratwurst and Sauerkraut\n",
      "        Currency: Euro"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='AI: \\n        Here is what I know:\\n        Capital: Berlin\\n        Language: German\\n        Food: Bratwurst and Sauerkraut\\n        Currency: Euro')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate \n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 낮을수록 창의적\n",
    "    streaming=True,  # 실시간으로 글자 보여줘 \n",
    "    callbacks=[StreamingStdOutCallbackHandler()]) # console에 보이는 문자를 다 print해줘 (다양한 llm에서 일어나는 event를 감지하는 방법)\n",
    "\n",
    "#example\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "#format examples\n",
    "# example_template = \"\"\"Human: {question} AI : {answer}\"\"\"\n",
    "# example_prompt = PromptTemplate.from_template(example_template)\n",
    "# or just \n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\n AI : {answer}\")\n",
    "\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples,\n",
    "    suffix = \"Human: What do you know about {country}?\", #example 다 넣고, question of user\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "#prompt.format(country=\"Germany\")\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"country\":\"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        I know this:\n",
      "        Capital: Berlin\n",
      "        Language: German\n",
      "        Food: Bratwurst and Sauerkraut\n",
      "        Currency: Euro\n",
      "        "
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='\\n        I know this:\\n        Capital: Berlin\\n        Language: German\\n        Food: Bratwurst and Sauerkraut\\n        Currency: Euro\\n        ')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chatprompt templete 사용해보기\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 낮을수록 창의적\n",
    "    streaming=True,  # 실시간으로 글자 보여줘 \n",
    "    callbacks=[StreamingStdOutCallbackHandler()]) # console에 보이는 문자를 다 print해줘 (다양한 llm에서 일어나는 event를 감지하는 방법)\n",
    "\n",
    "#example\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\", # 참고\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt =  ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"human\", \"What do you know about {country}?\"), #위 참고 가 country\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "\n",
    "example_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples,\n",
    ")\n",
    "\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert\"),\n",
    "    example_prompt,\n",
    "    (\"human\", \"What do you know about {country}?\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | chat\n",
    "chain.invoke({\"country\":\"Germany\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about France?\\n AI : \\n        Here is what I know:\\n        Capital: Paris\\n        Language: French\\n        Food: Wine and Cheese\\n        Currency: Euro\\n        \\n\\nHuman: What do you know about Brazil?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Dynamic example selector\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector # 예제의 양을 얼마나 되는지 확인하고, 세팅값에 따라 알맞은 prompt예제 골라줄 것임.\n",
    "\n",
    "        \n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1, # 낮을수록 창의적\n",
    "    streaming=True,  # 실시간으로 글자 보여줘 \n",
    "    callbacks=[StreamingStdOutCallbackHandler()]) # console에 보이는 문자를 다 print해줘 (다양한 llm에서 일어나는 event를 감지하는 방법)\n",
    "\n",
    "#example\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\n AI : {answer}\")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=80,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate( \n",
    "    example_prompt = example_prompt,\n",
    "    example_selector=example_selector, #추가 (이제는 전체 예제를 사용하는게 아니라 selector를 거친 예제들만)\n",
    "    suffix = \"Human: What do you know about {country}?\", \n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "\n",
    "prompt.format(country='Brazil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: What do you know about France?\\nAI:\\n        Here is what I know:\\n        Capital: Paris\\n        Language: French\\n        Food: Wine and Cheese\\n        Currency: Euro\\n        \\n\\nHuman: What do you know about Brazil?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Dynamic example selector\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import example_selector\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI:{answer}\")\n",
    "\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    example_selector=example_selector,\n",
    "    suffix=\"Human: What do you know about {country}?\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Brazil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization and Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Germany'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to load the prompt templates from the disc. \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"./prompt.json\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of Germany'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to load the prompt templates from the disc. \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"./prompt.yaml\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt.format(country=\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    \\n    You are a role playing assistant.\\n    And you are impersonating a Pirate\\n\\n                                     \\n    \\n    This is an example of how you talk:\\n\\n    Human: What is your location?\\n    You: Arrrrg! That is a secret!! Arg arg!!\\n\\n                              \\n    \\n    Start now!\\n\\n    Human: What is your fav food?\\n    You:\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to compose many prompt together \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant.\n",
    "    And you are impersonating a {character}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "\n",
    "    Human: {example_question}\n",
    "    You: {example_answer}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "\n",
    "    Human: {question}\n",
    "    You:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "                                     \n",
    "    {example}\n",
    "                              \n",
    "    {start}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start),\n",
    "]\n",
    "\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final,\n",
    "    pipeline_prompts=prompts,\n",
    ")\n",
    "\n",
    "#이거 끄고 아래꺼 돌려봐\n",
    "full_prompt.format(\n",
    "    character= \"Pirate\",\n",
    "    example_question= \"What is your location?\",\n",
    "    example_answer= \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "    question= \"What is your fav food?\",\n",
    ")\n",
    "\n",
    "\n",
    "# chain = full_prompt | chat\n",
    "\n",
    "# chain.invoke(\n",
    "#     {\n",
    "#         \"character\": \"Pirate\",\n",
    "#         \"example_question\": \"What is your location?\",\n",
    "#         \"example_answer\": \"Arrrrg! That is a secret!! Arg arg!!\",\n",
    "#         \"question\": \"What is your fav food?\",\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: How do you make italian pasta\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [3.50s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it becomes smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it becomes smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 13,\n",
      "      \"completion_tokens\": 255,\n",
      "      \"total_tokens\": 268\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it becomes smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caching allows save the responses from llm. Save the questions and answers and re-use it when it is asked again. Save money. \n",
    "# Cache we are using is memory cache which means once we restart the notebook, it will be gone. \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug \n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache()) # responses from llm, will be in cache memory\n",
    "set_debug(True) #shows more detail what is going on\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- Pinch of salt\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add a pinch of salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into desired shapes, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes, or until al dente.\\n9. Drain the pasta and toss with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only took 0.2s\n",
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: How do you make italian pasta\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [3.74s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 13,\n",
      "      \"completion_tokens\": 254,\n",
      "      \"total_tokens\": 267\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQLiteCache\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.globals import set_llm_cache, set_debug \n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"langchain.db\")) # name anything \n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: How do you make italian pasta\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:llm:ChatOpenAI] [2ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To make Italian pasta, you will need the following ingredients:\\n\\n- 2 cups of all-purpose flour\\n- 2 large eggs\\n- 1/2 teaspoon of salt\\n- Water (if needed)\\n\\nHere is a step-by-step guide to making Italian pasta:\\n\\n1. On a clean work surface, pour the flour and make a well in the center.\\n2. Crack the eggs into the well and add the salt.\\n3. Using a fork, gradually mix the eggs into the flour until a dough forms.\\n4. Knead the dough for about 10 minutes until it is smooth and elastic. If the dough is too dry, add a little water. If it is too wet, add a little more flour.\\n5. Wrap the dough in plastic wrap and let it rest for at least 30 minutes.\\n6. After resting, roll out the dough using a pasta machine or a rolling pin until it is thin.\\n7. Cut the dough into your desired shape, such as fettuccine or spaghetti.\\n8. Cook the pasta in a large pot of boiling salted water for 2-3 minutes or until al dente.\\n9. Drain the pasta and toss it with your favorite sauce or toppings.\\n\\nEnjoy your homemade Italian pasta!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"How do you make italian pasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "#How much money are we spending on each call\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "set_debug(False) \n",
    "with get_openai_callback() as usage: # usage 라고 안써도괜찮아\n",
    "    chat.predict(\"What is the recope for soju\")\n",
    "    print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients:\n",
      "- 1 bottle of soju\n",
      "- Ice cubes\n",
      "- Water or soda water (optional)\n",
      "- Fresh fruit or herbs for garnish (optional)\n",
      "\n",
      "Instructions:\n",
      "1. Chill the bottle of soju in the refrigerator for at least 2 hours before serving.\n",
      "2. Fill a glass with ice cubes.\n",
      "3. Pour the desired amount of soju over the ice cubes.\n",
      "4. If desired, top off with water or soda water to dilute the alcohol content.\n",
      "5. Garnish with fresh fruit or herbs for added flavor and presentation.\n",
      "6. Stir well before serving.\n",
      "7. Enjoy your refreshing soju cocktail! Ingredients:\n",
      "- 3 1/2 cups all-purpose flour\n",
      "- 1 packet active dry yeast\n",
      "- 1 1/2 cups warm water\n",
      "- 2 tablespoons sugar\n",
      "- 1 teaspoon salt\n",
      "- 2 tablespoons olive oil\n",
      "\n",
      "Instructions:\n",
      "1. In a small bowl, dissolve the yeast in warm water and let it sit for about 5 minutes until it becomes frothy.\n",
      "2. In a large mixing bowl, combine the flour, sugar, and salt. Make a well in the center and pour in the yeast mixture and olive oil.\n",
      "3. Mix the ingredients together until a dough forms. Knead the dough on a floured surface for about 10 minutes until it becomes smooth and elastic.\n",
      "4. Place the dough in a greased bowl, cover it with a clean towel, and let it rise in a warm place for about 1 hour or until it has doubled in size.\n",
      "5. Punch down the dough and shape it into a loaf. Place the loaf in a greased loaf pan, cover it, and let it rise for another 30 minutes.\n",
      "6. Preheat the oven to 375°F (190°C). Bake the bread for about 30-35 minutes or until it is golden brown and sounds hollow when tapped on the bottom.\n",
      "7. Remove the bread from the oven and let it cool on a wire rack before slicing and serving. Enjoy your freshly baked bread! \n",
      "\n",
      "Tokens Used: 303\n",
      "\tPrompt Tokens: 14\n",
      "\tCompletion Tokens: 289\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0005989999999999999\n"
     ]
    }
   ],
   "source": [
    "#How much money are we spending on each call\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    # streaming=True,\n",
    "    # callbacks=[\n",
    "    #     StreamingStdOutCallbackHandler(),\n",
    "    # ],\n",
    ")\n",
    "set_debug(False) \n",
    "with get_openai_callback() as usage: # usage 라고 안써도괜찮아\n",
    "    a = chat.predict(\"What is the recope for soju\")\n",
    "    b = chat.predict(\"What is the recope for bread\")\n",
    "    print(a,b,\"\\n\")\n",
    "    print(usage)\n",
    "    \n",
    "#아래 request가 2개 이지만, 지금 langchain.db가 돌아가고 있어서 1개로 만 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Serialization (DOES NOT WORK ANYMORE)\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.llms.openai import OpenAI\n",
    "# from langchain.llms.loading import load_llm\n",
    "\n",
    "\n",
    "# chat = ChatOpenAI(\n",
    "#     temperature=0.1,\n",
    "#     max_tokens=450,\n",
    "#     model=\"gpt-3.5-turbo-16k\"\n",
    "# )\n",
    "\n",
    "# chat.save(\"model.json\") #model저장\n",
    "# #chat = load_llm(\"model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation Buffer Memory : save whole conversation.\n",
    "# use when text completion, but for chat model, you need \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1. Create memory (chat purpose or not)\n",
    "memory = ConversationBufferMemory(return_messages=True) #return_messages를 넣어줘야지 Chat용도\n",
    "# 2. Save\n",
    "memory.save_context({\"input\":\"Hi!\"},{\"output\":\"How are you?\"})\n",
    "# 3. load\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?'),\n",
       "  HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\":\"Hi!\"},{\"output\":\"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?'),\n",
       "  HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?'),\n",
       "  HumanMessage(content='Hi!'),\n",
       "  AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\":\"Hi!\"},{\"output\":\"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "# it keeps store the same thing over and over agian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConversationBufferWindowMemory (up to a point ex) last 5 message) \n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4\n",
    ")\n",
    "\n",
    "def add_message(input,output):\n",
    "    memory.save_context({\"input\":input},{\"output\":output})\n",
    "    \n",
    "add_message(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(2,2)\n",
    "add_message(3,3)\n",
    "add_message(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(5,5)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConversationSummaryMemory : \n",
    "# It doesn't save messages as they come, instead it makes summary of the conversation. \n",
    "\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def add_message(input,output):\n",
    "    memory.save_context({\"input\":input},{\"outputs\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Hi, I am Chulwon, I live in Chicago\",\"Wow! That is so cool!\")\n",
    "add_message(\"Chicago is so pretty\",\"I wish I could go!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Chulwon from Chicago introduces themselves to the AI, who responds enthusiastically. The human mentions how pretty Chicago is, and the AI expresses a desire to visit the city.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConversationSummaryBufferMemory :\n",
    "# It keeps count of how many messages we have sending to the memory.\n",
    "# Once it hit the limit, it will start summarizing the old messages.\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limt=15, # how many maximum tokens can our messages be before they become summarized. \n",
    "    return_messages=True\n",
    ") \n",
    "\n",
    "def add_message(input,output):\n",
    "    memory.save_context({\"input\":input},{\"outputs\":output})\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Hi, I am Chulwon, I live in Chicago\",\"Wow! That is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I am Chulwon, I live in Chicago'),\n",
       "  AIMessage(content='Wow! That is so cool!')]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Chicago is so pretty\",\"I wish I could go!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I am Chulwon, I live in Chicago'),\n",
       "  AIMessage(content='Wow! That is so cool!'),\n",
       "  HumanMessage(content='Chicago is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!')]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is Chicago from Atlanta?\",\"I don't know! Not that far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I am Chulwon, I live in Chicago'),\n",
       "  AIMessage(content='Wow! That is so cool!'),\n",
       "  HumanMessage(content='Chicago is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!'),\n",
       "  HumanMessage(content='How far is Chicago from Atlanta?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\")]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is New Yrok from Korea?\",\"I don't know! Not that far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I am Chulwon, I live in Chicago'),\n",
       "  AIMessage(content='Wow! That is so cool!'),\n",
       "  HumanMessage(content='Chicago is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!'),\n",
       "  HumanMessage(content='How far is Chicago from Atlanta?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\"),\n",
       "  HumanMessage(content='How far is New Yrok from Korea?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\")]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is Tokyo from Korea?\",\"I don't know! Not that far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I am Chulwon, I live in Chicago'),\n",
       "  AIMessage(content='Wow! That is so cool!'),\n",
       "  HumanMessage(content='Chicago is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!'),\n",
       "  HumanMessage(content='How far is Chicago from Atlanta?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\"),\n",
       "  HumanMessage(content='How far is New Yrok from Korea?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\"),\n",
       "  HumanMessage(content='How far is Tokyo from Korea?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\")]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I am Chulwon, I live in Chicago'),\n",
       "  AIMessage(content='Wow! That is so cool!'),\n",
       "  HumanMessage(content='Chicago is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!'),\n",
       "  HumanMessage(content='How far is Chicago from Atlanta?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\"),\n",
       "  HumanMessage(content='How far is New Yrok from Korea?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\"),\n",
       "  HumanMessage(content='How far is Tokyo from Korea?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\"),\n",
       "  HumanMessage(content='How far is Rome from Korea?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\")]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"How far is Rome from Korea?\",\"I don't know! Not that far!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I am Chulwon, I live in Chicago'),\n",
       "  AIMessage(content='Wow! That is so cool!'),\n",
       "  HumanMessage(content='Chicago is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!'),\n",
       "  HumanMessage(content='How far is Chicago from Atlanta?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\"),\n",
       "  HumanMessage(content='How far is New Yrok from Korea?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\"),\n",
       "  HumanMessage(content='How far is Tokyo from Korea?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\"),\n",
       "  HumanMessage(content='How far is Rome from Korea?'),\n",
       "  AIMessage(content=\"I don't know! Not that far!\"),\n",
       "  HumanMessage(content='what is the tallest building in Korea?'),\n",
       "  AIMessage(content=\"I don't know!!\")]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"what is the tallest building in Korea?\",\"I don't know!!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConversationKGMemory : it builds knowledge graph of the entity\n",
    "\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Chulwon, I live in Chicago\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Chulwon: Chulwon lives in Chicago.')]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is Chulwon?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Chulwon likes steak\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Chulwon: Chulwon lives in Chicago. Chulwon likes steak.')]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"inputs\": \"what does Chulwon like\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory on LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mMy name is Chulwon\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Chulwon! How can I assist you today?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLMChain : off the shelf chain(general purpose chain) but hard to customize.\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=80,\n",
    ")\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.predict(question='My name is Chulwon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\", the capital city of South Korea. It is a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic temples. I love exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views from Namsan Mountain. Seoul is a dynamic city that never fails to impress me with its energy and charm.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question='I live in Seoul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question='What is my name?')\n",
    "\n",
    "#verbose true로 보니까 history 가 없어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human introduces themselves as Chulwon and mentions they live in Seoul, the capital city of South Korea. The AI responds by describing Seoul as a bustling metropolis with a vibrant culture, delicious food, and a mix of modern skyscrapers and historic temples. The AI expresses their love for exploring the city's neighborhoods, trying new restaurants, and taking in the beautiful views from Namsan Mountain, highlighting Seoul's dynamic energy and charm.\\nHuman: What is my name?\\nAI: I'm sorry, I do not have access to personal information such as your name.\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리는 업데이트가 된거는 맞아..the only problem is llm is not being given the history of the conversation.\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is Chulwon\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Chulwon! How can I assist you today?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",#; 메모리는 잘 저장되는 것을 위에 확인 했으니, 아래 템플릿에 위 memory를 잘 넣어줘야함 \n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question='My name is Chulwon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Chulwon\n",
      "AI: Hello Chulwon! How can I assist you today?\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know! How can I assist you with anything related to Seoul or any other topic today?\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question='I live in Seoul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Chulwon\n",
      "AI: Hello Chulwon! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to know! How can I assist you with anything related to Seoul or any other topic today?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Chulwon.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question='What is my name?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatBased Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Chulwon\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Chulwon! How can I assist you today?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mmeoery classes can be output in 2 format. 1. string 2. messages \n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True #dont turn it into string\n",
    ")\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), #누가 보냈는지 알 수 없는, 예측하기 어려운 메세지의 양과 제한없는 양의 메새지를 가질수 있음. \n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question='My name is Chulwon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL Based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more control to memory setting\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm #\n",
    "# 1. load memory()돌아가면서, 그 output이 history 에 들어가고  \n",
    "# 2. 그게 사용자의 input과 결합되면서 prompt에 전달\n",
    "\n",
    "def invoke_chain(question): # 이게 필요한 이유는, 우리가 메모리 관리를 수동으로 하고 있기 떄문에 우리는 사용자와 기계간의 인터렉션을 메모리에 저쟁해야함\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, Chulwon! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is Chulwon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Chulwon.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of Memory\n",
    "* Purpose is adding the history to the prompt that LLM gets.\n",
    "* LLM has off the shelf(automatic) \n",
    "* Langchain expression : manual but more customize\n",
    "\n",
    "3 ways for Memeory\n",
    "- 1. LLMChain\n",
    "- 2. Chatprompt template\n",
    "- 3. Manual (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20079dd2ac21832bb0e8ff352d23b6c3837ca9c91609572aa5b291e173833a08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
